{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BERT\n",
    "[优秀BLOG](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 0. 理论知识\n",
    "### 研究原因\n",
    "作者认为现有技术（关于利用预训练 LM 的表征，处理下游任务）限制了 LM 的表征能力\n",
    "> 现有 LM 模型是单向的，这限制了在预训练期间可以使用的架构\n",
    "#### 具体例子\n",
    "GPT 使用的是 left-to-right 架构，这导致每个单词仅仅只能对其 previous tokens 进行 **Attention**\n",
    "> 作者认为，这种单向的结构在 Sentence-level tasks 中是次优的，例如在 QA 问题中，一个 token 的context 来自两个方向\n",
    "\n",
    "---\n",
    "\n",
    "### CLS token\n",
    "- 每个 Sentence 的第一个 token 都是 [CLS]\n",
    "- [CLS] 对应的 Final hidden state is used as the aggregate sequence representation for **classification task**\n",
    "#### CLS in Huggingface\n",
    "[doc](https://huggingface.co/transformers/model_doc/bert.html#bertmodel)\n",
    "\n",
    "cls 对应的**output hidden state**是 outputs.pooler_output 或者使用 outputs[1] \n",
    "\n",
    "在 huggingface 中指出：BERT-family of models 中的 pooler_output 是 $h_{cls}$ 进行进一步处理得到的值：将其依次送入 **a linear layer**, **a tanh activation function**. 其中 **linear layer**的 weights 是在预训练中的 NSP 过程中得到的\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Pre-training BERT\n",
    "\n",
    "### Masked Language Modeling\n",
    "- 以一定的概率对输入的 tokens 用 Special token [mask] 进行替换，模型需要预测出 **these masked tokens**\n",
    "\n",
    "### Next Sentence Prediction\n",
    "- 对于输入：*Sentence A* [SEP] *Sentence B*, 判断这两个句子是否是连续的，即 Sentence B 是否是 Sentence A 的下一句话\n",
    "\n",
    "## Prediction\n",
    "- BERT uses the output embedding of the unique token [CLS] at the beginning of each such sentence for prediction\n",
    "\n",
    "## 2. Fine-tuning BERT\n",
    "对于 BERT 的输出\n",
    "- The token representation 输入到 **Output layer** for token-level tasks\n",
    "> Sequence tagging, question answering\n",
    "\n",
    "- CLS representation 输入到 **Output layer** for classification\n",
    "> Sentiment analysis, entailment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RoBERTa\n",
    "\n",
    "## 特点\n",
    "1. 与 BERT 有相同的结构\n",
    "2. 在预训练阶段，移除了 NSP\n",
    "3. 增加了 Data size 和 Batch size"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}